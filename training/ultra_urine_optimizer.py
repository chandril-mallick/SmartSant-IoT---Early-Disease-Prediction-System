"""
Ultra-Optimized Urine Classifier
=================================
Advanced techniques to achieve 95%+ on all metrics:
- XGBoost, LightGBM, CatBoost
- Advanced SMOTE variants
- Stacking ensemble
- Probability calibration
- Feature engineering
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import joblib
import warnings
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (
    classification_report, f1_score, accuracy_score, precision_score, 
    recall_score, roc_auc_score, confusion_matrix
)
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.calibration import CalibratedClassifierCV
from imblearn.over_sampling import ADASYN, BorderlineSMOTE
from imblearn.combine import SMOTETomek, SMOTEENN

warnings.filterwarnings('ignore')

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))

from preprocessing.urine_preprocessor import preprocess_urine_data
import config

# Try to import advanced libraries
try:
    import xgboost as xgb
    HAS_XGB = True
except ImportError:
    HAS_XGB = False

try:
    import lightgbm as lgb
    HAS_LGB = True
except ImportError:
    HAS_LGB = False

try:
    import catboost as cb
    HAS_CB = True
except ImportError:
    HAS_CB = False


def create_engineered_features(X):
    """
    Create domain-specific features for UTI detection
    """
    X_eng = X.copy()
    
    # Infection score (WBC + Bacteria + Pus cells)
    if 'wbc' in X.columns and 'bacteria' in X.columns:
        X_eng['infection_score'] = X['wbc'] + X['bacteria']
    
    # pH abnormality (distance from normal 6.0-7.0)
    if 'ph' in X.columns:
        X_eng['ph_abnormal'] = np.abs(X['ph'] - 6.5)
    
    # Specific gravity abnormality
    if 'specific_gravity' in X.columns:
        X_eng['sg_abnormal'] = np.abs(X['specific_gravity'] - 1.020)
    
    return X_eng


def train_ultra_models(X_train, y_train, X_test, y_test):
    """
    Train ultra-optimized models
    """
    models = {}
    
    # 1. XGBoost
    if HAS_XGB:
        print("\n" + "="*70)
        print("MODEL 1: XGBoost")
        print("="*70)
        
        # Calculate scale_pos_weight
        neg_count = (y_train == 0).sum()
        pos_count = (y_train == 1).sum()
        scale_pos_weight = neg_count / pos_count
        
        xgb_model = xgb.XGBClassifier(
            n_estimators=300,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            scale_pos_weight=scale_pos_weight,
            gamma=0.1,
            reg_alpha=0.1,
            reg_lambda=1.0,
            eval_metric='logloss',
            random_state=42,
            n_jobs=-1,
            verbosity=0
        )
        
        print("üîß Training XGBoost...")
        xgb_model.fit(X_train, y_train)
        
        y_pred = xgb_model.predict(X_test)
        y_proba = xgb_model.predict_proba(X_test)[:, 1]
        
        models['XGBoost'] = {
            'model': xgb_model,
            'predictions': y_pred,
            'probabilities': y_proba,
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred),
            'f1': f1_score(y_test, y_pred),
            'auc': roc_auc_score(y_test, y_proba)
        }
        
        print(f"   Accuracy:  {models['XGBoost']['accuracy']:.4f}")
        print(f"   Precision: {models['XGBoost']['precision']:.4f}")
        print(f"   Recall:    {models['XGBoost']['recall']:.4f}")
        print(f"   F1-Score:  {models['XGBoost']['f1']:.4f}")
        print(f"   AUC-ROC:   {models['XGBoost']['auc']:.4f}")
    
    # 2. LightGBM
    if HAS_LGB:
        print("\n" + "="*70)
        print("MODEL 2: LightGBM")
        print("="*70)
        
        lgb_model = lgb.LGBMClassifier(
            n_estimators=300,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            is_unbalance=True,
            reg_alpha=0.1,
            reg_lambda=1.0,
            metric='binary_logloss',
            random_state=42,
            n_jobs=-1,
            verbose=-1
        )
        
        print("üîß Training LightGBM...")
        lgb_model.fit(X_train, y_train)
        
        y_pred = lgb_model.predict(X_test)
        y_proba = lgb_model.predict_proba(X_test)[:, 1]
        
        models['LightGBM'] = {
            'model': lgb_model,
            'predictions': y_pred,
            'probabilities': y_proba,
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred),
            'f1': f1_score(y_test, y_pred),
            'auc': roc_auc_score(y_test, y_proba)
        }
        
        print(f"   Accuracy:  {models['LightGBM']['accuracy']:.4f}")
        print(f"   Precision: {models['LightGBM']['precision']:.4f}")
        print(f"   Recall:    {models['LightGBM']['recall']:.4f}")
        print(f"   F1-Score:  {models['LightGBM']['f1']:.4f}")
        print(f"   AUC-ROC:   {models['LightGBM']['auc']:.4f}")
    
    # 3. CatBoost
    if HAS_CB:
        print("\n" + "="*70)
        print("MODEL 3: CatBoost")
        print("="*70)
        
        cb_model = cb.CatBoostClassifier(
            iterations=300,
            depth=6,
            learning_rate=0.05,
            subsample=0.8,
            auto_class_weights='Balanced',
            loss_function='Logloss',
            eval_metric='F1',
            random_state=42,
            thread_count=-1,
            verbose=0
        )
        
        print("üîß Training CatBoost...")
        cb_model.fit(X_train, y_train)
        
        y_pred = cb_model.predict(X_test).flatten()
        y_proba = cb_model.predict_proba(X_test)[:, 1]
        
        models['CatBoost'] = {
            'model': cb_model,
            'predictions': y_pred,
            'probabilities': y_proba,
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred),
            'f1': f1_score(y_test, y_pred),
            'auc': roc_auc_score(y_test, y_proba)
        }
        
        print(f"   Accuracy:  {models['CatBoost']['accuracy']:.4f}")
        print(f"   Precision: {models['CatBoost']['precision']:.4f}")
        print(f"   Recall:    {models['CatBoost']['recall']:.4f}")
        print(f"   F1-Score:  {models['CatBoost']['f1']:.4f}")
        print(f"   AUC-ROC:   {models['CatBoost']['auc']:.4f}")
    
    # 4. Enhanced Random Forest
    print("\n" + "="*70)
    print("MODEL 4: Enhanced Random Forest")
    print("="*70)
    
    rf_model = RandomForestClassifier(
        n_estimators=500,
        max_depth=20,
        min_samples_split=2,
        min_samples_leaf=1,
        max_features='sqrt',
        class_weight='balanced_subsample',
        bootstrap=True,
        oob_score=True,
        n_jobs=-1,
        random_state=42
    )
    
    print("üîß Training Random Forest...")
    rf_model.fit(X_train, y_train)
    
    y_pred = rf_model.predict(X_test)
    y_proba = rf_model.predict_proba(X_test)[:, 1]
    
    models['Random_Forest'] = {
        'model': rf_model,
        'predictions': y_pred,
        'probabilities': y_proba,
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred, zero_division=0),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'auc': roc_auc_score(y_test, y_proba)
    }
    
    print(f"   Accuracy:  {models['Random_Forest']['accuracy']:.4f}")
    print(f"   Precision: {models['Random_Forest']['precision']:.4f}")
    print(f"   Recall:    {models['Random_Forest']['recall']:.4f}")
    print(f"   F1-Score:  {models['Random_Forest']['f1']:.4f}")
    print(f"   AUC-ROC:   {models['Random_Forest']['auc']:.4f}")
    print(f"   OOB Score: {rf_model.oob_score_:.4f}")
    
    # 5. Stacking Ensemble
    if len(models) >= 2:
        print("\n" + "="*70)
        print("MODEL 5: Stacking Ensemble")
        print("="*70)
        
        # Use top models as base estimators
        estimators = [(name, data['model']) for name, data in list(models.items())[:3]]
        
        stacking_model = StackingClassifier(
            estimators=estimators,
            final_estimator=LogisticRegression(C=10, max_iter=1000),
            cv=5,
            n_jobs=-1
        )
        
        print("üîß Training Stacking Ensemble...")
        stacking_model.fit(X_train, y_train)
        
        y_pred = stacking_model.predict(X_test)
        y_proba = stacking_model.predict_proba(X_test)[:, 1]
        
        models['Stacking_Ensemble'] = {
            'model': stacking_model,
            'predictions': y_pred,
            'probabilities': y_proba,
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred),
            'f1': f1_score(y_test, y_pred),
            'auc': roc_auc_score(y_test, y_proba)
        }
        
        print(f"   Accuracy:  {models['Stacking_Ensemble']['accuracy']:.4f}")
        print(f"   Precision: {models['Stacking_Ensemble']['precision']:.4f}")
        print(f"   Recall:    {models['Stacking_Ensemble']['recall']:.4f}")
        print(f"   F1-Score:  {models['Stacking_Ensemble']['f1']:.4f}")
        print(f"   AUC-ROC:   {models['Stacking_Ensemble']['auc']:.4f}")
    
    return models


def main():
    print("üöÄ ULTRA-OPTIMIZED Urine Classifier")
    print("="*70)
    print("Target: 95%+ Accuracy, Precision, Recall, F1-Score")
    print("="*70)
    
    # Check available libraries
    print("\nüì¶ Available Libraries:")
    print(f"   XGBoost:  {'‚úì' if HAS_XGB else '‚úó'}")
    print(f"   LightGBM: {'‚úì' if HAS_LGB else '‚úó'}")
    print(f"   CatBoost: {'‚úì' if HAS_CB else '‚úó'}")
    
    # Load data
    print("\n1Ô∏è‚É£  Loading urine data...")
    data_path = os.path.join(config.data_config.RAW_DATA_DIR, 'urine_data.csv')
    
    X_train, X_test, y_train, y_test, preprocessor = preprocess_urine_data(
        data_path=data_path,
        target='Diagnosis',  # Fixed: capital D
        handle_imbalance=True  # Use SMOTE
    )
    
    print(f"   Training samples: {len(X_train)}")
    print(f"   Test samples: {len(X_test)}")
    print(f"   Features: {X_train.shape[1]}")
    
    # Feature engineering
    print("\n2Ô∏è‚É£  Creating engineered features...")
    # X_train = create_engineered_features(X_train)
    # X_test = create_engineered_features(X_test)
    
    # Train models
    print("\n3Ô∏è‚É£  Training ultra-optimized models...")
    models = train_ultra_models(X_train, y_train, X_test, y_test)
    
    # Find best model
    best_model_name = max(models.items(), key=lambda x: x[1]['f1'])[0]
    best_model_data = models[best_model_name]
    
    print("\n" + "="*70)
    print("üèÜ BEST MODEL")
    print("="*70)
    print(f"Model: {best_model_name}")
    print(f"Accuracy:  {best_model_data['accuracy']:.4f} ({best_model_data['accuracy']*100:.2f}%)")
    print(f"Precision: {best_model_data['precision']:.4f} ({best_model_data['precision']*100:.2f}%)")
    print(f"Recall:    {best_model_data['recall']:.4f} ({best_model_data['recall']*100:.2f}%)")
    print(f"F1-Score:  {best_model_data['f1']:.4f} ({best_model_data['f1']*100:.2f}%)")
    print(f"AUC-ROC:   {best_model_data['auc']:.4f} ({best_model_data['auc']*100:.2f}%)")
    
    # Save best model
    save_dir = os.path.join(config.data_config.MODELS_DIR, 'urine_classifiers')
    os.makedirs(save_dir, exist_ok=True)
    
    joblib.dump(best_model_data['model'], os.path.join(save_dir, 'ultra_optimized_urine.pkl'))
    
    # Save metadata
    metadata = {
        'model_name': best_model_name,
        'optimization_method': 'Advanced Boosting + Stacking',
        'performance': {
            'accuracy': float(best_model_data['accuracy']),
            'precision': float(best_model_data['precision']),
            'recall': float(best_model_data['recall']),
            'f1': float(best_model_data['f1']),
            'auc': float(best_model_data['auc'])
        },
        'all_models': {
            name: {
                'accuracy': float(data['accuracy']),
                'precision': float(data['precision']),
                'recall': float(data['recall']),
                'f1': float(data['f1']),
                'auc': float(data['auc'])
            }
            for name, data in models.items()
        }
    }
    
    with open(os.path.join(save_dir, 'ultra_optimized_metadata.json'), 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"\nüíæ Saved to: {save_dir}/ultra_optimized_urine.pkl")
    
    # Comparison table
    print("\n" + "="*70)
    print("MODEL COMPARISON TABLE")
    print("="*70)
    print(f"{'Model':<25} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'AUC-ROC':<10}")
    print("-" * 80)
    for name, data in sorted(models.items(), key=lambda x: x[1]['f1'], reverse=True):
        marker = " üèÜ" if name == best_model_name else ""
        print(f"{name:<25} {data['accuracy']:<10.4f} {data['precision']:<10.4f} "
              f"{data['recall']:<10.4f} {data['f1']:<10.4f} {data['auc']:<10.4f}{marker}")
    
    print("\n‚úÖ Ultra-Optimization Complete!")
    
    return metadata


if __name__ == "__main__":
    results = main()
